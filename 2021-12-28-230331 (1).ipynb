{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Scoring for classifiers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Classifiers from scikit-learn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#%%\n",
    "attrition = pd.read_csv('attrition.csv')\n",
    "attrition.shape #(1470*35)\n",
    "attrition.isnull().sum()#check for missing value, find none.\n",
    "attrition.describe()\n",
    "attrition.info()\n",
    "attrition.corr()\n",
    "attrition.hist(figsize = (16,20),bins = 50, xlabels=8, ylabels = 8)\n",
    "\n",
    "#%% obtain data X and target y.\n",
    "attrition = pd.read_csv('attrition.csv')\n",
    "\n",
    "y_0 = attrition.iloc[:,1]\n",
    "X_right= attrition.iloc[:,2:35]\n",
    "X_left = attrition.iloc[:,0]\n",
    "X_1 = pd.concat([X_left,X_right], axis = 1)\n",
    "\n",
    "y = np.where(y_0 =='Yes',1,0)\n",
    "\n",
    "\n",
    "# I don't know how to integrate onhotencoder into pandas dataframe.\n",
    "#onehot_encoder = OneHotEncoder(sparse=False)\n",
    "#onehot_encoded = onehot_encoder.fit_transform(X_1[['BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime']])\n",
    "\n",
    "X_2 = pd.concat([X_1,pd.get_dummies(X_1[['BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime']])],axis=1)\n",
    "\n",
    "X = X_2.drop(['BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime'],axis=1)\n",
    "\n",
    "X.shape #1470*55\n",
    "\n",
    "#need to use one_hot_encode, and normalization(put all data into same bell curve),\n",
    "\n",
    "#%% feature selection: only select the feature that's useful.\n",
    "\n",
    "#see zhihu post on feature engineering.\n",
    "\n",
    "\n",
    "#%% decision tree, use default parameters, get a baseline.\n",
    "\n",
    "DTg = DecisionTreeClassifier(criterion = 'entropy')\n",
    "\n",
    "\n",
    "f1_cv= cross_val_score(DTg, X, y, scoring='f1_weighted', cv=10)\n",
    "f1_avg = sum(f1_cv)/len(f1_cv)\n",
    "print('f1',f1_avg)\n",
    "\n",
    "acc_cv= cross_val_score(DTg, X, y, scoring='accuracy', cv=10)\n",
    "acc_avg = sum(acc_cv)/len(acc_cv)\n",
    "print('accuracy',acc_avg)\n",
    "\n",
    "auc_cv= cross_val_score(DTg, X, y, scoring='roc_auc', cv=10)\n",
    "auc_avg = sum(auc_cv)/len(auc_cv)\n",
    "print('AUC',auc_avg)\n",
    "\n",
    "#%% tune parameter of Decision tree: max leaf nodes\n",
    "\n",
    "#need to add more parameters.\n",
    "#add cartesian grid search to look for best parameter.\n",
    "\n",
    "#stoch....gradient descent model to boost the data.(like adaboost)\n",
    "\n",
    "leaf_li = [3,4,5,6,7,8,9,10,20,50,100,200,500]\n",
    "DTig_li = [DecisionTreeClassifier(criterion = 'entropy', max_leaf_nodes = leaf_li[i]) for i in range(len(leaf_li))]\n",
    "\n",
    "# Train the classifier using 10-fold training data, pruducing F measure,\n",
    "#and take average as final output score.\n",
    "f1_dtig_li =[]\n",
    "for leaf, DTig in zip(leaf_li, DTig_li):    \n",
    "    f1_cv_dtig= cross_val_score(DTig, X, y, scoring='f1_weighted', cv=10)\n",
    "    f1_dtig = sum(f1_cv_dtig)/len(f1_cv_dtig)\n",
    "    f1_dtig_li.append(f1_dtig)\n",
    "    print('k',leaf,\"f1\", f1_dtig)\n",
    "\n",
    "#find optimal parameter (max leaf nodes) from the traing result\n",
    "def func(x):\n",
    "    return x[1]\n",
    "\n",
    "f1_DTig_sort = sorted(list(zip(leaf_li,f1_dtig_li)), key=func, reverse=True)\n",
    "best_k_ig = f1_DTig_sort[0][0]\n",
    "print('best k, info gain:',best_k_ig) #best k seems to be 7. with f1 score 0.8259.\n",
    "\n",
    "#%% tune parameter: max depth\n",
    "\n",
    "depth_li = np.linspace(1, 32, 32, endpoint=True)\n",
    "DTdep_li = [DecisionTreeClassifier(criterion = 'entropy', max_depth = depth_li[i],max_leaf_nodes = 7) for i in range(len(leaf_li))]\n",
    "# note: if not add max_leaf_node, the best depth is 2 with 0.824 f1 score; \n",
    "#if added max_leaf_node =7, the best depth is 4 with 0.8259 f1 score.\n",
    "\n",
    "\n",
    "# Train the classifier using 10-fold cross validation, producing F measure (f1),\n",
    "#and take average as final output score.\n",
    "f1_dep_li =[]\n",
    "for depth, DTdep in zip(depth_li, DTdep_li):    \n",
    "    f1_cv_dep= cross_val_score(DTdep, X, y, scoring='f1_weighted', cv=10)\n",
    "    f1_dep_avg = sum(f1_cv_dep)/len(f1_cv_dep)\n",
    "    f1_dep_li.append(f1_dep_avg)\n",
    "    print('max depth',depth,\"f1\", f1_dep_avg)\n",
    "\n",
    "#find optimal parameter (max leaf nodes) from the traing result\n",
    "def func(x):\n",
    "    return x[1]\n",
    "\n",
    "f1_dep_sort = sorted(list(zip(depth_li,f1_dep_li)), key=func, reverse=True)\n",
    "best_dep_ig = f1_dep_sort[0][0]\n",
    "print('best depth, info gain:',best_dep_ig) \n",
    "#with max_leaf_node =7, the best depth is 4, f1=0.8259\n",
    "\n",
    "DTdep47 = DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, max_leaf_nodes = 7)\n",
    "\n",
    "DTdep47.fit(X,y)\n",
    "\n",
    "print(DTdep47.feature_importances_)\n",
    "\n",
    "#%% random forest, basically all default parameters.\n",
    "woods = RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "woods.fit(X, y)\n",
    "#result: when 100 trees in forest: f1 = 0.815 accuracy = 0.856 AUC = 0.79\n",
    "#when 200 trees in forest: f1 = 0.810 accuracy = 0.855 AUC = 0.80\n",
    "#when 500 trees in forest: f1= 0.815 acc = 0.857 AUC = 0.81\n",
    "\n",
    "\n",
    "woodf1_cv= cross_val_score(woods, X, y, scoring='f1_weighted', cv=5)\n",
    "woodf1_avg = sum(woodf1_cv)/len(woodf1_cv)\n",
    "print('f1',woodf1_avg)\n",
    "\n",
    "wood_acc_cv= cross_val_score(woods, X, y, scoring='accuracy', cv=5)\n",
    "wood_acc_avg = sum(wood_acc_cv)/len(wood_acc_cv)\n",
    "print('accuracy',wood_acc_avg)\n",
    "\n",
    "wood_auc_cv= cross_val_score(woods, X, y, scoring='roc_auc', cv=5)\n",
    "wood_auc_avg = sum(wood_auc_cv)/len(wood_auc_cv)\n",
    "print('AUC',wood_auc_avg)\n",
    "\n",
    "#%% random forest, random grid search parameter tuning\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#set the parameters\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = [9,10,11,12,13,14,15] \n",
    "\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 30, num=10)]\n",
    "\n",
    "\n",
    "max_leaf_nodes = [100,200,500,1000,2000,3000]\n",
    "\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2,3,5,10,20]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [False]\n",
    "\n",
    "num_combo = len(n_estimators)*len(max_features)*len(max_depth) \\\n",
    "            *len(max_leaf_nodes)*len(min_samples_split)*len(min_samples_leaf)\n",
    "\n",
    "print('tot combo:', num_combo)\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'max_leaf_nodes':max_leaf_nodes,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "#pp.pprint(random_grid)\n",
    "\n",
    "# Use the random grid to search for best parameters\n",
    "\n",
    "# originaly, there are 12960 types of parameter combination. \n",
    "#we first randomly select 100 of combination, to narrow down search scope.\n",
    "\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations.\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=0, n_jobs = -1)\n",
    "#note: The most important arguments in RandomizedSearchCV are n_iter, \n",
    "#which controls the number of different combinations to try.\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X, y)\n",
    "\n",
    "#gave out the best parameters among the 100 random combination. \n",
    "print(rf_random.best_params_)\n",
    "print(rf_random.best_score_) \n",
    "\n",
    "#%% result from above, randam state = 42\n",
    "#first run: \n",
    "#{'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, \n",
    "#'max_leaf_nodes': 70, 'max_features': 'sqrt', 'max_depth': 14, 'bootstrap': False}\n",
    "#score = 0.8578231292517007\n",
    " \n",
    "\n",
    "#second run\n",
    "#'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, \n",
    "#'max_leaf_nodes': 100, 'max_features': 'sqrt', 'max_depth': 15, 'bootstrap': False \n",
    "#score = 0.861904761904762\n",
    "\n",
    "#3rd run\n",
    "#'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, \n",
    "#'max_leaf_nodes': 150, 'max_features': 'auto', 'max_depth': 12, 'bootstrap': False\n",
    "#score = 0.863265306122449\n",
    "\n",
    "#4th run\n",
    "#{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, \n",
    " #'max_leaf_nodes': 150, 'max_features': 'auto', 'max_depth': 13, 'bootstrap': True}\n",
    "# score = 0.8585034013605443 with bootstrap true.\n",
    "\n",
    "\n",
    "#5th run\n",
    "#{'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, \n",
    "# 'max_leaf_nodes': 200, 'max_features': 'auto', 'max_depth': 11, 'bootstrap': False}\n",
    "# score = 0.8612244897959184\n",
    " \n",
    "#6th run : didn't restrict max_features, got lower score \n",
    "#{'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, \n",
    "#'max_leaf_nodes': None,'max_features': None, 'max_depth': 10, 'bootstrap': False}\n",
    "#0.8204081632653061\n",
    " \n",
    "#7th run\n",
    "#'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, \n",
    "#'max_leaf_nodes': 200, 'max_features': 'auto', 'max_depth': 14, 'bootstrap': False}\n",
    "#score = 0.8646258503401361 \n",
    " \n",
    "#8th run\n",
    "#{'n_estimators': 350, 'min_samples_split': 3, 'min_samples_leaf': 1, \n",
    "# 'max_leaf_nodes': 500, 'max_features': 12, 'max_depth': 15, 'bootstrap': False}\n",
    "# 0.8673469387755102 \n",
    "\n",
    "#9th run \n",
    "#{'n_estimators': 250, 'min_samples_split': 3, 'min_samples_leaf': 1, \n",
    "#'max_leaf_nodes': 100, 'max_features': 12, 'max_depth': 10, 'bootstrap': False}\n",
    "# 0.8666666666666667 \n",
    " \n",
    " #10th run\n",
    "#{'n_estimators': 550, 'min_samples_split': 2, 'min_samples_leaf': 2, \n",
    "#'max_leaf_nodes': None, 'max_features': 15, 'max_depth': 11, 'bootstrap': False}\n",
    "#0.8659863945578231\n",
    "\n",
    "# !! good 11th run\n",
    "#{'n_estimators': 550, 'min_samples_split': 5, 'min_samples_leaf': 2, \n",
    "#'max_leaf_nodes': 150, 'max_features': 12, 'max_depth': 10, 'bootstrap': False}\n",
    "#0.8680272108843538\n",
    "\n",
    "#set max leaf nodes to None, everythingelse same.\n",
    "#{'n_estimators': 550, 'min_samples_split': 5, 'min_samples_leaf': 2, \n",
    "# 'max_leaf_nodes': None, 'max_features': 12, 'max_depth': 10, 'bootstrap': False}\n",
    "#0.8625850340136054 \n",
    " \n",
    "#12th run \n",
    "#{'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, \n",
    "#'max_leaf_nodes': None, 'max_features': 12, 'max_depth': 17, 'bootstrap': False}\n",
    "# 0.8659863945578231 \n",
    " \n",
    "# !!!!! good 13th run \n",
    "#'n_estimators': 350, 'min_samples_split': 5, 'min_samples_leaf': 2,\n",
    "# 'max_leaf_nodes': 250, 'max_features': 10, 'max_depth': 16, 'bootstrap': False}\n",
    "#0.8687074829931973\n",
    " \n",
    "#14th expand range of some parameter\n",
    "#{'n_estimators': 900, 'min_samples_split': 3, 'min_samples_leaf': 2, \n",
    "#'max_leaf_nodes': 2000, 'max_features': 13, 'max_depth': 15, 'bootstrap': False}\n",
    "#0.8673469387755102 \n",
    "\n",
    "#15th\n",
    "#'n_estimators': 600, 'min_samples_split': 3, 'min_samples_leaf': 2, \n",
    "#'max_leaf_nodes': 500, 'max_features': 11, 'max_depth': 16, 'bootstrap': False}\n",
    "#0.8673469387755102\n",
    " #%% narrow down the range of parameters using above result, and do a exhaustive grid search.\n",
    " \n",
    "from sklearn.model_selection import GridSearchCV\n",
    " \n",
    "narrow_grid = {'n_estimators': [1000],\n",
    "               'max_features': [13],\n",
    "               'max_depth': [12,13],\n",
    "               'max_leaf_nodes':[2000],\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [2],\n",
    "               'bootstrap': [False]}\n",
    "\n",
    "rf_narrow = GridSearchCV(estimator = rf, param_grid = narrow_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 1)\n",
    "\n",
    "rf_narrow.fit(X,y)\n",
    "\n",
    "print(rf_narrow.best_params_)\n",
    "#1st\n",
    "#'bootstrap': False, 'max_depth': 13, 'max_features': 13, 'max_leaf_nodes': 2000, \n",
    "#'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 1000\n",
    "#0.868027\n",
    "\n",
    "#2ed\n",
    "#{'bootstrap': False, 'max_depth': 12, 'max_features': 13, 'max_leaf_nodes': 2000, \n",
    "#'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 1400}\n",
    "#0.8666666666666667\n",
    "\n",
    "#3rd\n",
    "#'bootstrap': False, 'max_depth': 13, 'max_features': 13, 'max_leaf_nodes': 6000, \n",
    "#'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 1400}\n",
    "#0.8666666666666667\n",
    "\n",
    "#4th\n",
    "#{'bootstrap': False, 'max_depth': 12, 'max_features': 13, 'max_leaf_nodes': 10000, \n",
    "#'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 1400}\n",
    "#0.8646258503401361\n",
    "\n",
    "#'bootstrap': False, 'max_depth': 12, 'max_features': 13, 'max_leaf_nodes': 8000, \n",
    "#'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 1400}\n",
    "#0.8659863945578231\n",
    "\n",
    "#\n",
    "#{'bootstrap': False, 'max_depth': 12, 'max_features': 13, 'max_leaf_nodes': 7000, \n",
    " #'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 1400}\n",
    "#0.8659863945578231\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}